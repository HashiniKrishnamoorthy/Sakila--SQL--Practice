{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ece06e-0699-43b7-a3e9-9fa1ad1b346e",
   "metadata": {},
   "source": [
    "## üå≥ Decision Tree Regression ‚Äì Clear Explanation\n",
    "\n",
    "In **decision tree regression**, the goal is to predict a **continuous numerical value** (such as sales, price, or profit).  \n",
    "Instead of fitting one global equation, a decision tree works by **dividing the feature space into regions** and making simple predictions inside each region.\n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è‚É£ Dividing the Feature Space into Regions\n",
    "\n",
    "Suppose we have input features:\n",
    "\n",
    "\\[\n",
    "X = (X_1, X_2, \\dots, X_p)\n",
    "\\]\n",
    "\n",
    "The decision tree divides the feature space into **J distinct and non-overlapping regions**:\n",
    "\n",
    "\\[\n",
    "R_1, R_2, \\dots, R_J\n",
    "\\]\n",
    "\n",
    "**Key points about these regions:**\n",
    "- Each region is **distinct**\n",
    "- Regions **do not overlap**\n",
    "- Every observation belongs to **exactly one region**\n",
    "\n",
    "This division is done using a sequence of **if‚Äìelse splitting rules** based on feature values.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Prediction Inside a Region\n",
    "\n",
    "For any observation that falls into a region \\(R_j\\), the prediction is:\n",
    "\n",
    "\\[\n",
    "\\hat{y}_{R_j} = \\text{mean of all } y \\text{ values of training observations in } R_j\n",
    "\\]\n",
    "\n",
    "This means:\n",
    "- The model predicts a **constant value** inside each region\n",
    "- That constant value is the **average response** of the training data in that region\n",
    "\n",
    "üìå This is why decision tree regression produces **piecewise constant (step-like) predictions**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Why the Mean Is Used\n",
    "\n",
    "The mean is used because it **minimizes the squared error** within a region.\n",
    "\n",
    "For a single region, the value that minimizes:\n",
    "\n",
    "\\[\n",
    "\\sum (y_i - c)^2\n",
    "\\]\n",
    "\n",
    "is:\n",
    "\n",
    "\\[\n",
    "c = \\text{mean}(y)\n",
    "\\]\n",
    "\n",
    "Thus, the mean is the best possible prediction for each region under squared error loss.\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Objective Function (Residual Sum of Squares)\n",
    "\n",
    "The regions are chosen to minimize the **total residual sum of squares (RSS)**:\n",
    "\n",
    "\\[\n",
    "\\sum_{j=1}^{J} \\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(J\\) is the number of regions\n",
    "- \\(y_i\\) is the actual response value\n",
    "- \\(\\hat{y}_{R_j}\\) is the mean response of region \\(R_j\\)\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ How the Tree Chooses Splits\n",
    "\n",
    "At each step, the tree:\n",
    "1. Tries all possible splits across all features\n",
    "2. Computes the RSS before and after each split\n",
    "3. Chooses the split that **reduces RSS the most**\n",
    "\n",
    "This process is **greedy**, meaning it selects the best split at the current step without reconsidering previous splits.\n",
    "\n",
    "---\n",
    "\n",
    "### üîë Summary\n",
    "\n",
    "- Decision tree regression divides the feature space into non-overlapping regions\n",
    "- Each region predicts a **constant value**\n",
    "- That value is the **mean of the response variable** in the region\n",
    "- The tree is built by minimizing the **residual sum of squares**\n",
    "- The final model is a **piecewise constant approximation** of the true function\n",
    "\n",
    "---\n",
    "\n",
    "\\[\n",
    "X = (X_1, X_2,....., X_p)\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf2b801-13fa-4463-b529-bf9b691b939a",
   "metadata": {},
   "source": [
    "## üå≥ Recursive Binary Splitting and Tree Pruning\n",
    "\n",
    "### Recursive Binary Splitting (Greedy Approach)\n",
    "\n",
    "Decision trees use a **top-down greedy approach** called *recursive binary splitting* to divide the feature space.  \n",
    "Trying all possible partitions of the feature space is computationally infeasible, so the tree performs **binary splits** at each node.\n",
    "\n",
    "At each step, the algorithm selects:\n",
    "- a feature \\(X_j\\)\n",
    "- a split point \\(s\\)\n",
    "\n",
    "to divide the data into two regions:\n",
    "\n",
    "\\[\n",
    "R_1 = \\{x \\mid X_j < s\\}, \\quad R_2 = \\{x \\mid X_j \\ge s\\}\n",
    "\\]\n",
    "\n",
    "The optimal split minimizes the **Residual Sum of Squares (RSS)**:\n",
    "\n",
    "\\[\n",
    "\\sum_{i \\in R_1} (y_i - \\hat{y}_{R_1})^2 +\n",
    "\\sum_{i \\in R_2} (y_i - \\hat{y}_{R_2})^2\n",
    "\\]\n",
    "\n",
    "where \\(\\hat{y}_{R_1}\\) and \\(\\hat{y}_{R_2}\\) are the mean response values in the two regions.\n",
    "\n",
    "This approach is called **greedy** because it chooses the best split at the current step without considering future splits.\n",
    "\n",
    "Splitting continues recursively until a stopping criterion is met, after which predictions are made using the mean of the observations in each region.\n",
    "\n",
    "---\n",
    "\n",
    "### Overfitting in Decision Trees\n",
    "\n",
    "Recursive splitting can lead to highly complex trees that overfit the training data by creating very small regions with low bias but high variance.\n",
    "\n",
    "---\n",
    "\n",
    "### Tree Pruning (Regularization)\n",
    "\n",
    "Tree pruning reduces model complexity by penalizing large trees. The cost-complexity function is:\n",
    "\n",
    "\\[\n",
    "\\text{Cost}(T) = \\text{MSE}(T) + \\alpha |T|\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\(T\\) is a subtree of the full tree\n",
    "- \\(|T|\\) is the number of terminal nodes\n",
    "- \\(\\alpha \\ge 0\\) controls the penalty for complexity\n",
    "\n",
    "Cross-validation is used to select optimal values of \\(\\alpha\\) and \\(T\\).\n",
    "\n",
    "---\n",
    "\n",
    "### Post-Pruning (Backward Pruning)\n",
    "\n",
    "Post-pruning grows a full tree first and then removes non-significant branches.  \n",
    "A validation or cross-validation set is used to determine whether pruning improves performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Pre-Pruning (Forward Pruning)\n",
    "\n",
    "Pre-pruning stops tree growth early using conditions such as:\n",
    "- maximum tree depth\n",
    "- minimum samples per node\n",
    "- minimum impurity decrease\n",
    "\n",
    "This prevents unnecessary splits and reduces overfitting.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
